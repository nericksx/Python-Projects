{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS, VARIABLES & FUNCTIONS ##\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def export_and_write(\n",
    "        var_sprint, #sprint (2024-8) or backlog\n",
    "        var_ifExists #overwrite current db? (fail, replace, or append)\n",
    "    ):\n",
    "\n",
    "    if var_sprint == 'backlog':\n",
    "        var_table = 'Backlog'\n",
    "        var_JQL = 'project = \"UE\" AND issueType = \"story\" AND status = \"Backlog\"'\n",
    "        print(\"Backlog JQL:\", var_JQL)\n",
    "        \n",
    "    elif var_sprint != 'backlog':\n",
    "        var_table = 'Sprints'\n",
    "        var_JQL = 'project = \"UE\" AND issueType = \"story\" AND sprint = \"{}\"'.format(var_sprint)\n",
    "        #var_JQL = 'project = \"UE\" AND issueType = \"story\" AND status is not empty'\n",
    "        #var_JQL = 'key = \"UE-2216\"'\n",
    "        print(\"Sprint JQL:\", var_JQL)\n",
    "    else:\n",
    "        # Handle invalid table value\n",
    "        print(\"Invalid table name\")\n",
    "\n",
    "    # DB Connect\n",
    "    db_connection = create_engine('postgresql://postgres:mfwi3No9@localhost:5432/UE_Jira_data')\n",
    "\n",
    "    # RESTful API endpoint\n",
    "    url = 'https://foo.atlassian.net/rest/api/3/'\n",
    "\n",
    "    # Personal access token \n",
    "    token = 'foo123'\n",
    "    userID = 'foo@foo.com'\n",
    "\n",
    "    # Auth\n",
    "    auth = HTTPBasicAuth(userID, token)\n",
    "\n",
    "    # Set up request headers with the token\n",
    "    headers = {\n",
    "        'Accept': 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Function to fetch issues with pagination\n",
    "    def fetch_all_issues(url, var_JQL, headers, auth):\n",
    "        start_at = 0\n",
    "        max_results = 100  # Initial value\n",
    "        total_issues = float('inf')  # Initialize total_issues to positive infinity\n",
    "\n",
    "        all_issues = []\n",
    "\n",
    "        while len(all_issues) < total_issues:\n",
    "            # Make a GET request with headers and pagination parameters\n",
    "            response = requests.get(f'{url}/search?jql={var_JQL}', params={'maxResults': max_results, 'startAt': start_at}, headers=headers, auth=auth)\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                data = response.json()\n",
    "                #print(json.dumps(data, indent=4))\n",
    "\n",
    "                # Extract issues from the response\n",
    "                issues = data.get('issues', [])\n",
    "                count = len(issues)\n",
    "                print('Total issues retrieved in this batch:', count)\n",
    "\n",
    "                # Update total_issues if it's not set yet\n",
    "                if total_issues == float('inf'):\n",
    "                    total_issues = data.get('total', 0)\n",
    "\n",
    "                # Append the retrieved issues to the list\n",
    "                all_issues.extend(issues)\n",
    "\n",
    "                # Calculate the remaining issues\n",
    "                remaining_issues = total_issues - len(all_issues)\n",
    "\n",
    "                # Adjust max_results for the next request\n",
    "                max_results = min(remaining_issues, 100)\n",
    "\n",
    "                # Increment the startAt parameter for the next request\n",
    "                start_at += count\n",
    "                print('Start at:', start_at)\n",
    "\n",
    "                # Store the current start_at value\n",
    "                issue_total = start_at\n",
    "            \n",
    "            else:\n",
    "                print('Failed to fetch issues:', response.text)\n",
    "                break\n",
    "        \n",
    "        # Print the last start_at value\n",
    "        print('Total issues:', issue_total)\n",
    "        \n",
    "        # Append the current start_at to the issue_count list\n",
    "        issue_count.append(issue_total)\n",
    "\n",
    "        return all_issues\n",
    "     \n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    keys = []\n",
    "    issue_types = []\n",
    "    summaries = []\n",
    "    priorities = []\n",
    "    reporters = []\n",
    "    created_dates = []\n",
    "    in_progress_dates = []\n",
    "    completed_dates = []\n",
    "    statuses = []\n",
    "    labels = []\n",
    "    processes = []\n",
    "    methods = []\n",
    "    sprints = []\n",
    "    states = []\n",
    "    tshirt_sizes = []\n",
    "    story_points = []\n",
    "    issue_count = []\n",
    "\n",
    "    # Extract issues from the response\n",
    "    issues = fetch_all_issues(url, var_JQL, headers, auth)\n",
    "\n",
    "    # Function to return a single value from an input that is part of a list\n",
    "    def getListValue(issue, field_key):\n",
    "        field = issue.get('fields', {}).get(field_key, [])\n",
    "        if isinstance(field, list) and field:\n",
    "            # If the field is a list and not empty, join the names of all items in the list\n",
    "            return ', '.join(item.get('value', '') for item in field)\n",
    "        elif isinstance(field, dict):\n",
    "            # If the field is a dictionary, directly get the value\n",
    "            return field.get('value', '')\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    # Function to get state for most recent sprint\n",
    "    def getLatestSprintState(issue):\n",
    "        sprint_data = issue.get('fields', {}).get('customfield_10020', [])\n",
    "        latest_sprint_state = ''\n",
    "        latest_sprint_end_date = None\n",
    "\n",
    "        for sprint in sprint_data:\n",
    "            state = sprint.get('state', '')\n",
    "            end_date = sprint.get('endDate')\n",
    "            if end_date:\n",
    "                # Parse the end date to compare\n",
    "                end_date = pd.to_datetime(end_date)\n",
    "                # Check if this sprint ends after the latest one we've seen so far\n",
    "                if latest_sprint_end_date is None or end_date > latest_sprint_end_date:\n",
    "                    latest_sprint_end_date = end_date\n",
    "                    latest_sprint_state = state\n",
    "\n",
    "        return latest_sprint_state\n",
    "    \n",
    "    # Function to get sprint\n",
    "    def getSprintNames(issue):\n",
    "        sprint_data = issue.get('fields', {}).get('customfield_10020', [])\n",
    "        sprint_names = []\n",
    "\n",
    "        for sprint in sprint_data:\n",
    "            sprint_name = sprint.get('name', '')\n",
    "            if sprint_name:\n",
    "                sprint_names.append(sprint_name)\n",
    "\n",
    "        return sprint_names\n",
    "\n",
    "    # Function to get change log\n",
    "    def getChangeLog(issue_id):\n",
    "        response = requests.get(f'{url}/issue/{issue_id}/changelog', headers=headers, auth=auth)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON response\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f'Failed to fetch change log for {issue_id}:', response.text)\n",
    "            return None\n",
    "    \n",
    "    # Iterate over each issue in the response\n",
    "    for issue in issues:\n",
    "        # Extract key, summary, and reporter for each issue\n",
    "        key = issue.get('key', '')\n",
    "        type = issue.get('fields', {}).get('issuetype', {}).get('name', '')\n",
    "        summary = issue.get('fields', {}).get('summary', '')\n",
    "        reporter = issue.get('fields', {}).get('reporter', {}).get('displayName', '')\n",
    "        created = issue.get('fields', {}).get('created', '')\n",
    "        status = issue.get('fields', {}).get('status', {}).get('name', '')\n",
    "        label = issue.get('fields', {}).get('labels', '')\n",
    "        points = issue.get('fields', {}).get('customfield_10130', '')\n",
    "        tshirt = getListValue(issue,'customfield_10817')\n",
    "        process = getListValue(issue, 'customfield_10792')\n",
    "        method = getListValue(issue, 'customfield_10791')\n",
    "        try:\n",
    "            sprint = getSprintNames(issue)\n",
    "            state = getLatestSprintState(issue)\n",
    "        except TypeError:\n",
    "            # Handle the case where 'fields' key is not present\n",
    "            sprint = 'backlog'\n",
    "            state = 'todo'\n",
    "        try:\n",
    "            priority = issue.get('fields', {}).get('priority', {}).get('name', '')\n",
    "        except AttributeError:\n",
    "            # Handle the case where 'fields' key is not present\n",
    "            priority = 'Undefined'\n",
    "\n",
    "        # Initialize variables to store the timestamps\n",
    "        in_progress_date = None\n",
    "        completed_date = None\n",
    "\n",
    "        # Fetch the change log for the issue\n",
    "        change_log = getChangeLog(key)\n",
    "    \n",
    "        if change_log:  # Ensure change_log is not None\n",
    "            # Extract timestamps from the change log\n",
    "            for entry in change_log.get('values', []):\n",
    "                for item in entry.get('items', []):\n",
    "                    if item.get('field') == 'status':\n",
    "                        if item.get('toString') == 'IN PROGRESS':\n",
    "                            in_progress_date = entry.get('created', '')\n",
    "                        elif item.get('toString') in ['Completed', 'Accepted']:\n",
    "                            completed_date = entry.get('created', '')\n",
    "            \n",
    "        # Append data to lists\n",
    "        keys.append(key)\n",
    "        issue_types.append(type)\n",
    "        summaries.append(summary)\n",
    "        priorities.append(priority)\n",
    "        reporters.append(reporter)\n",
    "        created_dates.append(created)\n",
    "        in_progress_dates.append(in_progress_date)\n",
    "        completed_dates.append(completed_date)\n",
    "        statuses.append(status) \n",
    "        labels.append(label)\n",
    "        processes.append(process)\n",
    "        methods.append(method)\n",
    "        sprints.append(sprint)\n",
    "        states.append(state)\n",
    "        story_points.append(points)\n",
    "        tshirt_sizes.append(tshirt)\n",
    "\n",
    "    # Create DataFrame from the lists\n",
    "    df_issues = pd.DataFrame({\n",
    "        'Key': keys, \n",
    "        'Type': issue_types, \n",
    "        'Summary': summaries, \n",
    "        'Priority': priorities,\n",
    "        'T-Shirt Size' : tshirt_sizes,\n",
    "        'Story Points' : story_points,\n",
    "        'Reporter': reporters, \n",
    "        'Created': created_dates, \n",
    "        'Progress Start': in_progress_dates,\n",
    "        'Completed': completed_dates,\n",
    "        'Status': statuses, \n",
    "        'Sprint': sprints,\n",
    "        'Most Recent State' : states,\n",
    "        'Labels': labels, \n",
    "        'Process': processes, \n",
    "        'Method': methods \n",
    "    })\n",
    "    \n",
    "    # Convert a timestamp column to datetime and remove time\n",
    "    def convertDates(column):\n",
    "        df_issues[column] = pd.to_datetime(df_issues[column], yearfirst=True, utc=True).dt.date\n",
    "\n",
    "    convertDates('Created')\n",
    "    convertDates('Progress Start')\n",
    "    convertDates('Completed')\n",
    "    \n",
    "    # Write the DF to a SQL DB\n",
    "    df_issues.to_sql(name=var_table, con=db_connection, if_exists=var_ifExists, index=False) \n",
    "\n",
    "    if var_sprint == 'backlog':\n",
    "        df_backlog_snapshot = pd.DataFrame({\n",
    "            'Date': datetime.today().strftime('%Y-%m-%d'),\n",
    "            'Issue Count': issue_count\n",
    "        })\n",
    "        df_backlog_snapshot.to_sql(name='Backlog Snapshop', con=db_connection, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sprint JQL: project = \"UE\" AND issueType = \"story\" AND sprint = \"2024-23\"\n",
      "Total issues retrieved in this batch: 53\n",
      "Start at: 53\n",
      "Total issues: 53\n"
     ]
    }
   ],
   "source": [
    "#export_and_write('2024-6', 'replace')\n",
    "#export_and_write('2024-7', 'append')\n",
    "#export_and_write('2024-8', 'append')\n",
    "#export_and_write('2024-9', 'append')\n",
    "#export_and_write('2024-10', 'append')\n",
    "#export_and_write('2024-11', 'append')\n",
    "#export_and_write('2024-12', 'append')\n",
    "#export_and_write('2024-13', 'append')\n",
    "#export_and_write('2024-14', 'append')\n",
    "#export_and_write('2024-15', 'append')\n",
    "#export_and_write('2024-16', 'append')\n",
    "#export_and_write('2024-17', 'append')\n",
    "#export_and_write('2024-18', 'append')\n",
    "#export_and_write('2024-19', 'append')\n",
    "#export_and_write('2024-20', 'append')\n",
    "#export_and_write('2024-21', 'append')\n",
    "#export_and_write('2024-22', 'append')\n",
    "export_and_write('2024-23', 'append')\n",
    "#export_and_write('backlog', 'replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
